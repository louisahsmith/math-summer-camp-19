---
title: "PHS 2000 Summer Camp"
subtitle: "Math Review"  
author: "Louisa H. Smith"
date: '`r Sys.Date()`'
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      beforeInit: "macros.js"
      ratio: 16:9
      highlightLines: true
      countIncrementalSlides: false
---
class:middle,center

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, width = 93)
knitr::opts_chunk$set(fig.retina = 3, out.width = "100%", fig.align = "center", message = FALSE, warning = FALSE)
knitr::opts_hooks$set(fig.callout = function(options) {
  if (options$fig.callout) {
    options$echo <- FALSE
    options$out.height <- "99%"
    options$fig.width <- 16
    options$fig.height <- 8
  }
  options
})
library(tidyverse)
reveal <- function(name, num) {
  content <- knitr:::knit_code$get(name)
  last_line <- which(str_detect(content, "\\+"))[num]
  if (is.na(last_line)) last_line <- length(content)
  if (num == 1) {
    first_line <- 1
  } else {
    first_line <- which(str_detect(content, "\\+"))[num - 1] + 1
  }
  content[last_line] <- str_remove(content[last_line], "\\+")
  new_lines <- paste0(content[first_line:last_line], " #<<")
  orig_lines <- if (num == 1) 0 else 1:(first_line - 1)
  c(content[orig_lines], new_lines)
}

set.seed(6789)
n <- 10
dat <- data.frame(
  age = round(runif(n, 22, 45)),
  height = round(rnorm(n, 66, 4)),
  likes_dogs = rbinom(n, 1, .75)
)
dat$yob <- 2019 - dat$age
dat <- as.matrix(dat)
v <- c(2, 4, 3)
w <- c(4, 1, 2)
X <- matrix(c(6, 3, 1, 6, 2, 3), ncol = 3)
```
```{css, echo=FALSE}
/* custom.css */
.title-slide {
  background-image: url("img/camp.svg");
  background-size: cover;
}

.left-code {
  #color: #777;
  width: 43%;
  height: 92%;
  float: left;
  #font-size: 0.8em;
  position: absolute;
}
.right-plot {
  width: 50%;
  float: right;
  padding-left: 5%;
}
.left-col {
  width: 60%;
  float: left;
  position: absolute;
}
.right-col {
  width: 30%;
  float: right;
  padding-left: 5%;
}
.plot-callout {
  height: 225px;
  width: 450px;
  bottom: 5%;
  right: 5%;
  position: absolute;
  padding: 0px;
  z-index: 100;
}
.plot-callout img {
  width: 100%;
  border: 4px solid #23373B;
}

h4 {
  color: #F97B64;
  font-size: 22px;
}

h1, h2, h3, h4 {
  margin-top:0;
}

.inverse h1, .inverse h2, .inverse h3 {
  color: #1F4257;
}
.remark-slide thead, .remark-slide tr:nth-child(2n) {
  background-color: white;
}
.title-slide, .title-slide h1, .title-slide h2, .title-slide h3 {
color: #FFFFFF;
}
```
```{r xaringan-themer, include=FALSE}
library(xaringanthemer)
duo_accent(
  primary_color = "#1F4257",
  secondary_color = "#F97B64",
  header_font_google = google_font("Lexend Deca"),
  text_font_google = google_font("Noto Sans")
)
```

# Index card:

## 1. How confident are you in your quantitative abilities?
### 1 = no confidence, 5 = very confident

---
class: inverse, middle, center

# How we read, store, represent data
## Scalars, vectors, matrices

---
class:center
.left[
# Scalars, vectors, matrices
]

$a = a \text{ , a scalar}$

<br>

--

$\mathbf{y} =\begin{bmatrix}y_1 \\ y_2 \\ \vdots \\ y_n\end{bmatrix}\text{,  a vector of length } n$

<br>

--

$\mathbf{X} = \begin{bmatrix} x_{11} & x_{12} & \cdots & x_{1p} \\ x_{21} & x_{22} & \cdots & x_{2p} \\ \vdots &\vdots & \ddots & \vdots \\x_{n1} & x_{n2} & \cdots & x_{np}\end{bmatrix} \text{,  an }n \times p \text{ matrix}$


---
### (Note on dimensions)
.right-col[
![](https://i3.cpcache.com/merchandise_zoom/206_550x550_Front_Color-NA.jpg?region={%22name%22:%22FrontCenter%22,%22width%22:20,%22height%22:12.18,%22alignment%22:%22MiddleCenter%22,%22orientation%22:0,%22dpi%22:150,%22crop_x%22:0,%22crop_y%22:0,%22crop_h%22:1800,%22crop_w%22:3000,%22scale%22:0,%22template%22:{%22id%22:16054111,%22params%22:{}}}&Size=NA&AttributeValue=NA)]

.left-col[
We read dimensions like $n \times p$ as **rows** by **columns**.

I remember this as **Rapid City** because I used to live in South Dakota. You may choose a mnemonic of your own!

<br>

**This is a 3 x 4 matrix:**

$\mathbf{X}_{3 \times 4} = \begin{bmatrix} x_{11} & x_{12} & x_{13} & x_{14}\\ x_{21} & x_{22} & x_{23} & x_{24}\\ x_{31} & x_{32} & x_{33} & x_{34} \end{bmatrix}_{3\times 4}$


If it's not clear from the context, we may write the dimensions out explicitly as above.

Individual elements are indexed in the same *row-column* order: $x_{23}$ is the element in the second row and third column.
]

---
# In "real life"
.center[
$a_{i} = 30, \text{ a single participant's age at baseline}$

<br>

$\mathbf{y} = \begin{bmatrix} 30 \\ 24 \\ \vdots \\ 46 \end{bmatrix} \text{,  ages at baseline of all the participants}$

$\mathbf{X} = \begin{bmatrix} 30 & 67 & \cdots & 1989 \\ 24 & 63 & \cdots & 1995 \\ \vdots &\vdots & \ddots & \vdots \\ 46 & 72 & \cdots & 1973 \end{bmatrix}$

$\text{values of all the variables (age at baseline, height in inches, ..., year of birth) for all participants}$
]

---
# In R

.center[
```{r, echo = FALSE}
dat
```

#### What are the dimensions of this matrix?
]

---
# Select a scalar
```{r, echo = FALSE}
dat
```
Extract a single number from the data:
```{r}
# participant 3's year of birth (the 4th variable)
dat[3, 4]
```
What will `dat[5, 2]` produce? What about `dat[2, 5]`?

---

# Select a vector
```{r, echo = FALSE}
dat
```
Extract a column from the data:
```{r}
# all participants' year of birth (the 4th variable)
dat[, 4]
```

---
# Row vectors

We can flip a vector on its side by **transposing** it.
<br>
<br>
.center[
$\mathbf{y} =\begin{bmatrix}y_1 \\ y_2 \\ \vdots \\ y_n\end{bmatrix}\text{,  a vector of length } n$

$\mathbf{y}^T =\begin{bmatrix}y_1 & y_2 & \cdots & y_n\end{bmatrix}\text{,  a row vector of length } n$
]

#### We may also refer to transposed vectors (and matrices) with "prime" notation: $\mathbf{y}^T = \mathbf{y}'$
(Not to be confused with a derivative!)
---

# Select a row vector
```{r, echo = FALSE}
dat
```
Extract a row from the data:
```{r}
# all information about participant 5
dat[5, ]
```

---
## (Note on dimensions)

We can also think of vectors as single-columned matrices:

- A column vector $\mathbf{y}$ of length $n$ has dimensions $n \times 1$.
.center[
$\begin{bmatrix}y_1 \\ y_2 \\ \vdots \\ y_n\end{bmatrix}_{n\times 1}$
]


- A row vector $\mathbf{y}'$  of length $n$ has dimensions $1 \times n$.
.center[
$\begin{bmatrix}y_1 & y_2 & \cdots & y_n\end{bmatrix}_{1\times n}$
]

#### Thinking about vectors this way will be really handy when we multiply vectors and matrices!

---

# We can also transpose matrices
<br>

$\mathbf{X} = \begin{bmatrix} 30 & 67 & \cdots & 1989 \\ 24 & 63 & \cdots & 1995 \\ \vdots &\vdots & \ddots & \vdots \\ 46 & 72 & \cdots & 1973 \end{bmatrix}\qquad \qquad \qquad \qquad \mathbf{X}^T = \mathbf{X}' = \begin{bmatrix} 30 & 24 & \cdots & 46 \\ 67 & 63 & \cdots & 72 \\ \vdots &\vdots & \ddots & \vdots \\ 1989 & 1995 & \cdots & 1973 \end{bmatrix}$

<br>
<br>

The former columns (variables) are now rows.

The former rows (participants) are now columns.

**This will be handy mathematically (but you probably wouldn't just do it to your dataset for fun!).**

---
# In R
We can transpose a matrix using the `t()` function:
```{r}
t(dat)
```
If we save this as another object, we can extract elements using the **opposite** indices as before:

```{r}
dat_t <- t(dat)
dat[3, 4] # participant 3's year of birth
dat_t[4, 3]
```

---
class:inverse,middle,center
# Try it yourself

<!-- badges: start -->
[![Launch Rstudio exercises](http://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/louisahsmith/math-summer-camp-19/master?urlpath=rstudio)
<!-- badges: end -->

## Open the file `exercises-1.R` and complete the exercises

Remember to save and export your answers if you want to save them.

(If that doesn't work, download all files here: https://github.com/louisahsmith/math-summer-camp-19/archive/master.zip)

---
# What do these objects represent?

It can help to visualize what these objects mean geometrically:

Let's think of a vector of length 2, representing the age and height of participants 1 and 2:

$\mathbf{x}_1 = \begin{bmatrix} 30 \\ 67 \end{bmatrix} \qquad \qquad \mathbf{x}_2 = \begin{bmatrix} 31 \\ 66.5 \end{bmatrix}$

```{r, echo = FALSE}
library(plotly)
dat3d <- tibble(
  y = c(30, 31, 25, 35),
  z = c(67, 66.5, 60, 70),
  x = c(10.5, 7, 6, 11),
  color = c("red", "blue", "clear", "clear"),
  size = c(1, 1, 0, 0)
)

plot_ly(dat3d, x = ~y, y = ~z, color = ~color, colors = c("red", "white", "blue"), size = c(.5, .5, 0, 0), type = "scatter", mode = "markers", width = 300, height = 300) %>%
  layout(
    xaxis = list(title = "age"),
    yaxis = list(title = "height"),
    showlegend = FALSE,
    autosize = FALSE
  )
```

---
# Add another dimension

.right-col[
$\mathbf{x}_1 = \begin{bmatrix} 30 \\ 67 \\10.5 \end{bmatrix} \qquad \mathbf{x}_2 = \begin{bmatrix} 31 \\ 66.5 \\ 7 \end{bmatrix}$

<br>
<br>
.center[
#### The more attributes we measure about people, the higher-dimensional the space -- and the more precisely we can describe each person (they're not near anyone else in space).
]
]

```{r, echo = FALSE}
plot_ly(dat3d, x = ~x, z = ~z, y = ~y, color = ~color, colors = c("red", "white", "blue"), size = c(4, 4, 0, 0), type = "scatter3d", mode = "markers", width = 500, height = 500) %>%
  layout(
    scene = list(
      yaxis = list(title = "age"),
      xaxis = list(title = "shoe size"),
      zaxis = list(title = "height"),
      camera = list(
        up = list(x = 0, y = 0, z = 1),
        eye = list(x = 2.5, y = 0.1, z = 0.1),
        center = list(x = 0, y = 0, z = 0)
      )
    ),
    showlegend = FALSE
  )
```

---
# Multiplication

When we multiply a scalar by a vector or a matrix, we can just do so element by element:

$a\mathbf{y} = \begin{bmatrix} ay_1 \\ ay_2 \\ \vdots \\ ay_n \end{bmatrix} \qquad \qquad \qquad a\mathbf{X} = \begin{bmatrix} ax_{11} & ax_{12} & \cdots & ax_{1p} \\ ax_{21} & ax_{22} & \cdots & ax_{2p} \\ \vdots &\vdots & \ddots & \vdots \\ ax_{n1} & ax_{n2} & \cdots & ax_{np} \end{bmatrix}$

.center[
#### But multiplying vectors and matrices requires special rules
]

---

# Vector & matrix multiplication

We can only multiply vectors of the same length, and we have to **transpose** one before we can do so.

- Consider two vectors of length $p$, $\mathbf{b}$ and $\mathbf{c}$:

$\mathbf{b}^T\mathbf{c} = \begin{bmatrix} b_1 \\ b_2 \\ \vdots \\ b_p \end{bmatrix}^T \begin{bmatrix} c_1 \\ c_2 \\ \vdots \\ c_p \end{bmatrix} = \begin{bmatrix} b_1 & b_2 & \cdots & b_p \end{bmatrix} \begin{bmatrix} c_1 \\ c_2 \\ \vdots \\ c_p \end{bmatrix}$

$\qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad\qquad \qquad = b_1c_1 + b_2c_2 + \cdots + b_pc_p = \sum_{i = 1}^p b_ic_i$

<br>

.center[
#### I think of this as "flopping" $\mathbf{c}$ over directly on top of $\mathbf{b}$, and then summing all the products of the matching terms.
]

See why the vectors need to be the same length?

---
# Practice

Let's consider two vectors of length 3, $\mathbf{v}$ and $\mathbf{w}$:

$\mathbf{v}^T\mathbf{w} = \begin{bmatrix} 2 \\ 4 \\ 3\end{bmatrix}^T \begin{bmatrix} 4 \\ 1 \\ 2 \end{bmatrix} = \begin{bmatrix} 2 & 4 & 3 \end{bmatrix} \begin{bmatrix} 4 \\ 1 \\ 2 \end{bmatrix} = \; \; ?$

--

$\qquad \qquad \qquad\qquad \qquad \qquad\qquad \qquad \qquad(2 \times 4) + (4 \times 1) + (3 \times 2) = 18$

--

<br>
<br>

.center[
#### What is $\mathbf{w}^T\mathbf{v}$?
]

---
# In "real life"

One situation that occurs often in statistics is the product of a vector with itself:

$\mathbf{b}^T\mathbf{b} = \sum_{i = 1}^p b_i^2$

Matrix notation makes it easy to write down a sum of squares.


---
## In R
.pull-left[
Let's look at the two vectors (R prints them as rows but that's ok):
```{r}
v
w
```
We would hope to be able to multiply them... nope!
```{r}
v * w
```
#### Careful, this neither gives you an error message *nor* the right answer!
]

--

.pull-right[
Vector/ matrix multiplication in R requires a different operator: `%*%`

```{r}
t(v) %*% w
```
We can see that we get the same answer when we reverse the order:
```{r}
t(w) %*% v
```
]

---

# Vector $\times$ matrix multiplication

When we multiply a vector by a matrix, the same idea applies, only this time we have more rows or columns to sum over.

$\mathbf{X}\mathbf{b} = \begin{bmatrix} x_{11} & x_{12} & \cdots & x_{1p} \\ x_{21} & x_{22} & \cdots & x_{2p} \\ \vdots &\vdots & \ddots & \vdots \\ x_{n1} & x_{n2} & \cdots & x_{np} \end{bmatrix} \begin{bmatrix} b_1 \\ b_2 \\ \vdots \\ b_p \end{bmatrix} = \begin{bmatrix} x_{11}b_1 + x_{12}b_2 + \cdots x_{1p}b_p  \\ x_{21}b_1 + x_{22}b_2 + \cdots x_{2p}b_p  \\ \vdots \\ x_{n1}b_1 + x_{n2}b_2 + \cdots x_{np}b_p \end{bmatrix}$

We're **flopping** the column vector repeatedly onto the rows of the matrix, and summing the products that result.

.center[
![](img/flop.jpeg)
]

---
# Vector $\times$ matrix multiplication, cont.

Let's look at some other ways we could write this same product:

$\mathbf{X}\mathbf{b} = \begin{bmatrix} \mathbf{x_1}^T\mathbf{b}\\ \mathbf{x_2}^T\mathbf{b}\\ \vdots \\ \mathbf{x_n}^T\mathbf{b}\\ \end{bmatrix} = \begin{bmatrix} \sum_{i = 1}^p x_{1i}b_i\\ \sum_{i = 1}^p x_{2i}b_i\\ \vdots \\ \sum_{i = 1}^p x_{ni}b_i\\ \end{bmatrix}$

<br>

- The $\mathbf{x_1}^T\mathbf{b}$ form illustrates how each **element** of the final product is just a **vector-vector product**

- We're using the same $\mathbf{b}$ vector in each new element of our product, but a different one of the $n$ rows of $\mathbf{X}$ 
  - So we end up with $n$ elements in the resulting vector
  
---
## In R

.pull-left[
```{r}
X
v
w
```

#### What will be the dimensions of $\mathbf{Xv}$?
]

--

.pull-right[
We have to use the `%*%` operator again:

```{r}
X %*% v
X %*% w
```
]

---

# Matrix $\times$ matrix multiplication

Let's multiply a $m\times n$ matrix $\mathbf{Y}$ by the $n\times p$ matrix $\mathbf{X}$:

$\mathbf{Y}_{m\times n}\mathbf{X}_{n\times p} = \begin{bmatrix} \mathbf{y_{11}} & \mathbf{y_{12}} & \cdots & \mathbf{y_{1n}} \\ y_{21} & y_{22} & \cdots & y_{2n} \\ \vdots &\vdots & \ddots & \vdots \\ y_{m1} & y_{m2} & \cdots & y_{mn} \end{bmatrix} \begin{bmatrix} \mathbf{x_{11}} & x_{12} & \cdots & x_{1p} \\ \mathbf{x_{21}} & x_{22} & \cdots & x_{2p} \\ \vdots &\vdots & \ddots & \vdots \\ \mathbf{x_{n1}} & x_{n2} & \cdots & x_{np}\end{bmatrix} = \begin{bmatrix} \mathbf{z_{11}} &  & \cdots &  \\ &  & \cdots & \\ \vdots &\vdots & \ddots & \vdots \\  &  & \cdots &\end{bmatrix}$

where $\mathbf{z}_{11}$ is the product of the first **row** of $\mathbf{Y}$ and the first **column** of $\mathbf{Y}$

<br>

.center[
#### Now we need to lift out a single column vector before flopping
]

---
# Rinse and repeat
Now we do the same vector-vector multiplication with every pair of a row from $\mathbf{Y}$ and a column from $\mathbf{X}$


$\mathbf{Y}_{m\times n}\mathbf{X}_{n\times p} = \begin{bmatrix} \mathbf{y_{11}} & \mathbf{y_{12}} & \cdots & \mathbf{y_{1n}} \\ y_{21} & y_{22} & \cdots & y_{2n} \\ \vdots &\vdots & \ddots & \vdots \\ y_{m1} & y_{m2} & \cdots & y_{mn} \end{bmatrix} \begin{bmatrix} x_{11} & \mathbf{x_{12}} & \cdots & x_{1p} \\ x_{21} & \mathbf{x_{22}} & \cdots & x_{2p} \\ \vdots &\vdots & \ddots & \vdots \\ x_{n1} & \mathbf{x_{n2}} & \cdots & x_{np} \end{bmatrix} = \begin{bmatrix} z_{11} & \mathbf{z_{12}} & \cdots &  \\ &  & \cdots & \\ \vdots &\vdots & \ddots & \vdots \\ &  & \cdots & \end{bmatrix}$

(You don't have to do it in any particular order, just keep multiplying until all row-column pairs have been multiplied.)

$\qquad \qquad \; \; \; \,= \begin{bmatrix} y_{11} & y_{12} & \cdots & y_{1n} \\ \mathbf{y_{21}} & \mathbf{y_{22}} & \cdots & \mathbf{y_{2n}} \\ \vdots &\vdots & \ddots & \vdots \\ y_{m1} & y_{m2} & \cdots & y_{mn} \end{bmatrix} \begin{bmatrix} \mathbf{x_{11}} & x_{12} & \cdots & x_{1p} \\ \mathbf{x_{21}} & x_{22} & \cdots & x_{2p} \\ \vdots &\vdots & \ddots & \vdots \\ \mathbf{x_{n1}} & x_{n2} & \cdots & x_{np} \end{bmatrix} = \begin{bmatrix} z_{11} & z_{12} & \cdots &  \\ \mathbf{z_{21}} &  & \cdots & \\ \vdots &\vdots & \ddots & \vdots \\  &  & \cdots & \end{bmatrix}$

.center[
#### What will be the dimensions of $\mathbf{Z}$?
]

---
## (Note on dimensions)

Notice that I explicitly wrote out the dimensions when multiplying $\mathbf{Y}_{m\times n}\mathbf{X}_{n\times p}\,$

Just like we can only multiply vectors of the same length, dimensions of matrices must be compatible in order to be multiplied.

.center[
<iframe src="https://giphy.com/embed/ZDgXvr2vpsgZG" width="480" height="257" frameBorder="0" class="giphy-embed" allowFullScreen></iframe><p><a href="https://giphy.com/gifs/ZDgXvr2vpsgZG"></a></p>
]

The **inner dimensions** must match, then the resulting product is a matrix with the **outer dimensions**.

.center[
#### I think of this as the inner dimensions "collapsing"
]

---
# Multiplying matrices by themselves

As with vectors, we sometime need to transpose matrices to make the dimensions compatible.

Let's look at a new matrix $\mathbf{Q}_{2\times3}$:

$\mathbf{Q} = \begin{bmatrix} r & s & t \\ u & v & w \end{bmatrix} \qquad \qquad \qquad \mathbf{Q}^T = \begin{bmatrix} r & u \\ s & v \\ t & w \end{bmatrix}$

If we wanted to multiply $\mathbf{Q}$ by itself (similar to squaring a number), we can't just multiply $\mathbf{QQ}$ -- first we need to transpose one of the matrices to make the dimensions compatible:

$\mathbf{Q}^T\mathbf{Q} = \begin{bmatrix} r^2 + u^2 & rs + uv & rt + uw \\ rs + uv & s^2 + v^2 & st + vw \\ rt + uw & st + vw & t^2 + w^2 \end{bmatrix}$

.center[
#### What would be the dimensions of $\mathbf{Q}\mathbf{Q}^T$?
]

---
class:inverse,middle,center
# Try it yourself

<!-- badges: start -->
[![Launch Rstudio exercises](http://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/louisahsmith/math-summer-camp-19/master?urlpath=rstudio)
<!-- badges: end -->


## Open the file `exercises-2.R` and complete the exercises

Remember to save and export your answers if you want to save them.

---
# Identity matrix

We all know 1 is special. When you multiply a number by 1, you get the same number.

Matrices have their own special matrix, the identity matrix: $\mathbf{I}$.

.center[
$\mathbf{XI} = \mathbf{X} \text{ for any matrix } \mathbf{X}$
]

<br>

What does $\mathbf{I}$ look like? Let's look at what it would look like for $\mathbf{Q}_{2\times3}$.

.center[
#### What are the dimensions of $\mathbf{I}$?
]

--

$\mathbf{QI} = \begin{bmatrix} r & s & t \\ u & v & w \end{bmatrix}_{2\times3}\begin{bmatrix}   &   &  &  \\    &   &   & \\   &   &  &  \\ \end{bmatrix}_{3\times3} = \begin{bmatrix} r & s & t \\ u & v & w \end{bmatrix}_{2\times3}$

.center[
#### What could we put in the first column of $\mathbf{I}$ that would result in $r$ when multiplied by the first row of $\mathbf{Q}$? What would result in $s$, for the second column?
]

---

# Identity matrix revealed
.pull-left[
$\mathbf{I} = \begin{bmatrix} 1 & 0 & \cdots & 0 \\ 0 & 1 & 0 \cdots & 0 \\ \vdots &\vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 1 \end{bmatrix}$

#### What are the dimensions of $\mathbf{I}$?
]

.pull-right[
<iframe src="https://giphy.com/embed/6FzSjyMdrPm0M" width="480" height="480" frameBorder="0" class="giphy-embed" allowFullScreen></iframe><p><a href="https://giphy.com/gifs/the-oc-ryan-6FzSjyMdrPm0M"></a></p>
]

---
# Symmetric matrices

The identity matrix $\mathbf{I}$ is an example of a **symmetric** matrix.

<br>

The $\mathbf{Q}^T\mathbf{Q}$ matrix we saw earlier was as well:

.center[
$\mathbf{Q}^T\mathbf{Q} = \begin{bmatrix} r^2 + u^2 & rs + uv & rt + uw \\ rs + uv & s^2 + v^2 & st + vw \\ rt + uw & st + vw & t^2 + w^2 \end{bmatrix}$
]

<br>

- The element in the $1,2$ position is the same as that in the $2,1$ position, the element in the $1,3$ position is the same as that in the $3,1$ position, and so on
- The diagonal can be anything (1s for $\mathbf{I}$, and sums of squares for $\mathbf{Q}^T\mathbf{Q}$)

.center[
#### Correlation and covariance matrices are always symmetric
]

---
# Matrix inverses

An inverse of a function is kind of like the function that takes you back to some starting point.

- For a scalar $a$, $a^{-1}=\frac{1}{a}$ is the multiplicative inverse of $a$: when we multiply the two together, we get 1

- For a square matrix $\mathbf{X}$, there *sometimes* exists an **inverse matrix** $\mathbf{X}^{-1}$ such that $\mathbf{X}^{-1}\mathbf{X} = \mathbf{I}$ and $\mathbf{X}\mathbf{X}^{-1} = \mathbf{I}$

<br>

.center[
#### It can be tricky to find, and it doesn't always exist!
]

<br>

In particular, if the row vectors or column vectors that make up $\mathbf{X}$ are not **linearly independent**, then the inverse $\mathbf{X}^{-1}$ doesn't exist. 

---
# Intuition behind linear independence

Consider a situation in which we're predicting height as a linear function of several variables we have in our dataset: age in years, shoe size, and age in months.

.center[

$\textbf{height} = \begin{bmatrix} 51 \\ 61 \\ 52 \\ 65 \\ 60 \\ 48 \end{bmatrix} \qquad \qquad \textbf{other variables} = \begin{bmatrix} 30  & 7 & 1989 \\ 31 & 10 & 1988 \\ 25 & 9 & 1994 \\ 35 & 10 & 1984 \\ 42 & 6 & 1977 \\ 27 & 7 & 1992 \end{bmatrix}$

]

.pull-left[
We can plot predicted height as a function of age in years alone. This is a line.
]

.pull-right[

.center[

```{r, echo = FALSE}
set.seed(6789)
newdat <- tibble(
  age = c(30, 31, 25, 35, 42, 27),
  shoesize = round(runif(6, 6, 12)),
  height = round(age + shoesize * 3),
  yob =  age * 12
)

fit_age <- lm(height ~ age, data = newdat)
fit_age_shoe <- lm(height ~ age + shoesize, data = newdat)
fit_age_year <- lm(height ~ age + yob, data = newdat)

newdat <- newdat %>%
  mutate(
    p_age = fitted(fit_age),
    p_age_shoe = fitted(fit_age_shoe),
    p_age_year = fitted(fit_age_year)
  )

# predict over sensible grid of values
ages <- unique(newdat$age)
shoes <- unique(newdat$shoesize)
d <- with(newdat, expand.grid(age = ages, shoesize = shoes))
vals_shoes <- predict(fit_age_shoe, newdata = d)

# form matrix and give to plotly
m_shoes <- matrix(vals_shoes, nrow = length(unique(d$age)), ncol = length(unique(d$shoesize)))

plot_ly(newdat, x = ~age, y = ~p_age, type = "scatter", mode = "markers", width = 250, height = 250) %>%
  add_lines(x = ~age, y = ~p_age, color = I("lightgrey")) %>%
  layout(
    xaxis = list(title = "age in years"),
    yaxis = list(title = "predicted height"),
    showlegend = FALSE,
    autosize = FALSE
  )
```
]

]

---
# Linear independence, cont.

.pull-left[
If we plot predicted height as a function of age in years *and* shoe size, we get a **plane**.

<br>
<br>
<br>
<br>
<br>

That is, we get more information about height from knowing someone's shoe size. If two people are the same age, but have different shoe sizes, we'll predict different heights for them.
]

.pull-right[
.center[

```{r, echo = FALSE}
plot_ly(newdat, x = ~shoesize, z = ~p_age_shoe, y = ~age, type = "scatter3d", mode = "markers", width = 500, height = 500) %>%
  add_surface(x = ~shoes, y = ~ages, z = ~m_shoes, colorscale = list(c(0, 1), c("lightgrey", "lightgrey"))) %>%
  layout(
    scene = list(
      yaxis = list(title = "age in years"),
      xaxis = list(title = "shoe size"),
      zaxis = list(title = "predicted height"),
      camera = list(
        up = list(x = 0, y = 0, z = 1),
        eye = list(x = 2.5, y = 0.01, z = 0.2),
        center = list(x = 0, y = 0, z = 0.2)
      )
    ),
    showlegend = FALSE
  ) %>% hide_colorbar()
```
]
]


---
# Linear dependence

.pull-left[
But if we plot predicted height as a function of age in years and *age in months*, we get a line again, instead of a plane.

<br>
<br>


That's because age in years and age in months are **linearly dependent**: we can write one as a linear combination of the others; that is, age in months = 12 $\times$ age in years.

We don't get any extra information from knowing age in months that we didn't already have from age in years.
]

.pull-right[
.center[
```{r, echo = FALSE}
plot_ly(newdat, x = ~yob, z = ~p_age_year, y = ~age, type = "scatter3d", mode = "markers", width = 500, height = 500) %>%
  add_lines(x = ~yob, z = ~p_age_year, y = ~age, color = I("lightgrey")) %>%
  layout(
    scene = list(
      yaxis = list(title = "age in years"),
      xaxis = list(title = "age in months"),
      zaxis = list(title = "height"),
      camera = list(
        up = list(x = 0, y = 0, z = 1),
        eye = list(x = 2.5, y = 0.01, z = 0.2),
        center = list(x = 0, y = 0, z = 0.2)
      )
    ),
    showlegend = FALSE
  )
```
]
]

.center[
#### When else do you think linear dependence might be a problem?
]

---
# In "real life"

Linear dependence will come into play when we cover regression. In the meantime, we can practice some regression matrix notation! You may be used to seeing a linear regression equation written out like this:

.center[
$y_i = \beta_0 + \beta_1x_{1i} +\beta_1x_{2i} + \epsilon_i$
]

Well, that's the same thing as:

.center[
$y_i = \boldsymbol{\beta}^T\mathbf{x}_i + \epsilon_i$
]

where
$\;\boldsymbol{\beta} = \begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \end{bmatrix}\;$ and $\;\mathbf{x}_i = \begin{bmatrix} 1 \\ x_{1i} \\ x_{2i} \end{bmatrix}$

<br>

.center[
#### Try multiplying it out by hand, first making sure the dimensions are compatible, to be sure!
]

You'll review more regression material on Friday 😁

---
# In R
.pull-left[
We can attempt to invert a matrix using the `solve()` function:
```{r, echo = FALSE}
mat_a <- matrix(c(2, 6, 1, 8), ncol = 2)
```

```{r}
mat_a
solve(mat_a)
```
]


.pull-right[

We can check to make sure this is the inverse:

```{r}
mat_a_inv <- solve(mat_a)
mat_a_inv %*% mat_a
```
#### We get the identity matrix $\mathbf{I}$!
]

---
# In R, cont.

What if we have a matrix whose columns are not linearly independent?

```{r, echo = FALSE}
mat_b <- matrix(c(2, 6, 1, 3), ncol = 2)
```

```{r,error = TRUE}
mat_b
solve(mat_b)
```

#### Whenever you get an error message about something being "singular", that's code for a matrix not being invertible -- check for linear dependence! 

How could you tell the matrix above is not invertible?


---
class:middle

background-image: url("img/resources.jpg")

background-size:cover


# More resources

.pull-left[

[**This**](https://www.khanacademy.org/math/precalculus/precalc-matrices) whole section has a lot of great information and practice with matrices. 

For a more advanced introduction, work through the sections on vectors, linear combinations, and linear dependence [**here**](https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces). 

You can also pick and choose from the videos [**here**](https://www.khanacademy.org/math/linear-algebra/matrix-transformations), particularly those on functions and linear transformations.
]

---
class:middle, center
# Index card:

## 2. How confident are you feeling about vectors and matrices?
### 1 = no confidence, 5 = very confident

---
class: inverse, middle, center

# How we transform data

## Functions

---
# Logarithms

.left-code[
If you see $\log(x)$ in this class, or basically anywhere in probability and statistics, it will refer to the natural logarithm, or $\ln(x)$.

#### What do you notice about the function $\log(x)$?

<br>
<br>
<br>
<br>

You can only "log" a positive number. Something like $\log(-1)$ is undefined. We can see that $\lim_{x \to 0}\log(x) = -\infty$. Importantly, $\log(1) = 0$, so $\log(x)$ for any $x$ between 0 and 1 will give you a negative number.
]

.right-plot[

```{r logGraph, echo = F}
curve(log(x), xlim = c(0, 4), ylab = "log(x)", xlab = "x", main = "log(x)")
abline(h = 0, lty = "dashed")
points(1, 0, col = "red")
```
]

---
# Exponentiation
.left-code[
The **inverse** of the natural log is the natural expontial function $e^x$, which we also write as $\exp(x)$:

.center[
$\exp(x) = y \iff x = \log(y)$
]

so if one side of an equation is exponentiated, we can always "get out of it" by applying a logarithm to both sides, and vice versa.

.center[
$\log(\exp(x)) = x\;$ and $\;\exp(\log(x)) = x$
]

#### But we can't do that if x is negative! Recall that we can't take a log of a negative number, and exponentiating a number is never going to give us anything negative.
]

.right-plot[

```{r expGraph, echo = F}
curve(exp(x), xlim = c(-4, 4), ylab = "exp(x)", xlab = "x", main = "exp(x)")
abline(h = 0, lty = "dashed")
points(0, 1, col = "red")
```
]

---
# Rules to live by

When we have a sum inside an exponent, we can decompose this into the product of two exponents:
$$\exp(a + b) = \exp(a)\exp(b)$$
Similarly, a product inside a logarithm can be written as a sum of logs:
$$\log(ab) = \log(a) + \log(b)$$
Of course, the same is true of the inverses of addition and multiplication, subtraction and division:
$$\exp(a - b) = \frac{\exp(a)}{\exp(b)}$$
$$\log\left(\frac{a}{b}\right) = \log(a) - \log(b)$$

#### You will need to know these rules to understand various regression models in this class, I promise!

---
# Odds and probabilities
.pull-left[
Another important relationship! Think about flipping a coin lots of times: heads you win, tails you lose.

- A **probability** describes the number of successes out of the total number of trials (a proportion)
- An **odds** describes the number of successes compared the the number of failures (a ratio)

Let's say you get 4 heads out of 10 flips:

- probability = $\frac{4}{10}$
- odds = $\frac{4}{6}$

#### These are really different numbers!

]

.pull-right[
![](https://i0.wp.com/harvardsportsanalysis.org/wp-content/uploads/2015/11/CTAVzLbVAAAVm7B.jpg-large.jpg?w=1024)

<br>
<br>
<br>
.center[
$odds = \frac{prob}{1 - prob}$


$prob = \frac{odds}{1 + odds}$
]
]

---
# Logits and expits
.pull-left[
Now let's combine everything to make... the **logit** function!

![](img/recipe.gif)
]

.pull-right[

The **logit** of $p$, aka the **log-odds** of $p$, can take a number between 0 and 1 (like a probability!) and transform it to a number between $-\infty$ and $\infty$.

.center[
$logit(p) = \log\left(\frac{p}{1 - p}\right)$
]

We can invert it to get the **expit** function, which can take any number on the real line and transform it to a value between 0 and 1:

.center[
$expit(x) = \frac{\exp(x)}{1 + \exp(x)}$
]

]

---
# In real life

Consider the following logistic regression model for the probability that someone likes dogs ( $p_i$ ) given their age in years ( $x_i$ ):

$\log\left(\frac{p_i}{1-p_i}\right) = \beta_0 + \beta_1x_i$

#### What's the probability that a 30-year-old likes dogs?

--

.pull-left[
.center[

$\log\left(\frac{p_i}{1-p_i}\right) = \beta_0 + \beta_1\times30$

$\frac{p_i}{1-p_i} = \exp\left(\beta_0 + \beta_1\times30\right)$

$p_i = \frac{\exp\left(\beta_0 + \beta_1\times30\right)}{1 + \exp\left(\beta_0 + \beta_1\times30\right)}$
]
]

.pull-right[
![](https://hips.hearstapps.com/ghk.h-cdn.co/assets/17/30/1500925839-golden-retriever-puppy.jpg)
]

---
# In real life, cont.

Consider the following logistic regression model for the probability that someone likes dogs ( $p_i$ ) given their age in years ( $x_i$ ):

$\log\left(\frac{p_i}{1-p_i}\right) = \beta_0 + \beta_1x_i$


#### What's the odds ratio for a 5-year difference in age?

--

.pull-left[
.center[

$\frac{p_i}{1-p_i} = \exp\left(\beta_0 + \beta_1\times30\right)$

$\frac{p_j}{1-p_j} = \exp\left(\beta_0 + \beta_1\times35\right)$

$\frac{p_j}{1-p_j} \bigg/ \frac{p_i}{1-p_i} = \frac{\exp\left(\beta_0 + \beta_1\times35\right)}{\exp\left(\beta_0 + \beta_1\times30\right)}$

$\qquad\qquad\qquad\qquad= \frac{\exp(\beta_0)\exp(\beta_1\times35)}{\exp(\beta_0)\exp(\beta_1\times30)}$

$\qquad\qquad\qquad\qquad= \exp(\beta_1\times5)$
]
]


.pull-right[
![](https://d17fnq9dkz9hgj.cloudfront.net/uploads/2018/04/Beagle_02.jpg)
]

---

class:inverse,middle,center
# Try it yourself

<!-- badges: start -->
[![Launch Rstudio exercises](http://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/louisahsmith/math-summer-camp-19/master?urlpath=rstudio)
<!-- badges: end -->

## Open the file `exercises-3.R` and complete the exercises

Remember to save and export your answers if you want to save them.
---
class:middle

background-image: url("img/resources.jpg")

background-size:cover


# More resources

.pull-left[
Start [**here**](https://www.khanacademy.org/math/pre-algebra/pre-algebra-exponents-radicals/pre-algebra-exponent-properties/v/exponent-properties-involving-products) if you need more practice with exponents.

Try [**here**](https://www.khanacademy.org/math/algebra2/exponential-and-logarithmic-functions) for more practice with logarithms.
]

---
class:middle, center
# Index card:

## 3. How confident are you feeling about these types of functions?
### 1 = no confidence, 5 = very confident

---
class: inverse, middle, center

# How we analyze data

## Calculus

---

# Derivatives

.left-code[
The basic idea of a derivative is that it describes the rate of change of a function. If the function we're looking at is $g(x)$, then there are a couple of ways we usually notate the first derivative of $g(x)$, which we'll use interchangeably:
$$g'(x) = \frac{d}{dx}g(x)$$
Both equivalently tell us that we are looking at the function $g(x)$ and taking the first derivative with respect to the variable $x$. That means that as $x$ changes, we want to know how much $g(x)$ changes. This is just the slope of $g(x)$ at a given value of $x$.
]

.right-plot[
.center[

#### Where is $g'(x)$ greatest?

```{r slope1, echo = F}
curve(dexp(x, rate = 3), xlim = c(0, 2), ylab = "g(x)")
```

]
]

---

# More derivatives
.left-code[
.center[
#### Where is $g'(x)$ greatest?

```{r slope2, echo = F}
curve(dchisq(x, 4), xlim = c(1, 4), ylab = "g(x)")
```
]
]

--

.right-plot[
#### Where does $g'(x) = 0$?

When the first derivative is 0, the function may have reached its maximum or minimum.

So if you want to maximize a function, one way to do so is to differentiate it and then set it equal to 0.
]

---
# Integrals

.left-code[
We might want to describe a function by the area under its curve

An integral tells us how much cumulative space a function is covering (in terms of distance from the $x$-axis) as $x$ gets larger.
]

.right-plot[
In the graph of $f(x) = 2x^3 + 3x^2 + 4$ below, the area in blue is represented by the integral
$$\int_{-2}^{2} 2x^3 + 3x^2 + 4 \; dx$$
```{r index-3, echo = F, fig.height = 2, fig.width = 4, out.width = "70%"}
par(mar = c(2.1, 4.1, 0.1, 2.1))
func <- function(x) 2*x^3 + 3*x^2 + 4
curve(func, xlim = c(-2, 3), ylab = "f(x)")
coord_x <- c(-2, seq(-2, 2, length.out = 1000), 2)
coord_y <- c(0, func(seq(-2, 2, length.out = 1000)), 0)
polygon(coord_x, coord_y, col = "skyblue")
```
]

---

# Integrals, cont.

.left-code[
$$\int_{-2}^{2} 2x^3 + 3x^2 + 4 \; dx$$

The values at the top and bottom of the integral sign are those between which we're computing the integral. We could integrate over the whole function, from $- \infty$ to $\infty$, or choose other limits of integration:


```{r, echo = F, fig.height = 2, fig.width = 4, out.width = "70%"}

par(mar = c(2.1, 4.1, 0.1, 2.1))
func <- function(x) 2*x^3 + 3*x^2 + 4
curve(func, xlim = c(-2, 3), ylab = "f(x)")
coord_x <- c(1, seq(1, 3, length.out = 1000), 3)
coord_y <- c(0, func(seq(1, 3, length.out = 1000)), 0)
polygon(coord_x, coord_y, col = "purple")

```

#### What limits of integration are displayed here?
]

.right-plot[

The integral of a non-negative function increases with $x$:

$$\int_{-2}^{-1} 2x^3 + 3x^2 + 4 \; dx$$

$$\leq \int_{-2}^{0} 2x^3 + 3x^2 + 4 \; dx$$

$$\leq\int_{-2}^{1} 2x^3 + 3x^2 + 4 \; dx$$

$$\leq \int_{-2}^{2} 2x^3 + 3x^2 + 4 \; dx$$

and so on. On the graph, the area under the curve can only accumulate area, so the integral evaluated at greater and greater upper limits can only increase.

]

---
# In "real life"

.left-code[
At any given year of age, everyone still alive has some probability of dying.

Yearly risk of mortality changes over time; in particular, there's a spike within the first year of life, and then a constant increase through and after middle age.
]

.right-plot[
If we're interested in cumulative risk of death by some age, we'll have to **integrate** over the yearly risk from birth to that age.

![](https://www150.statcan.gc.ca/n1/pub/82-624-x/2014001/article/14009/c-g/c-g-02-eng.gif)
]

---
class:middle

background-image: url("img/resources.jpg")

background-size:cover


# More resources

.pull-left[
[**Here**](https://www.khanacademy.org/math/calculus-home/taking-derivatives-calc) is a **lot** of information about derivatives. You don't need more than the first few videos.

Same with [**this**](https://www.khanacademy.org/math/calculus-home/integral-calculus/definite-integrals-intro-ic) intro to integrals. 

Watch the video on antiderivatives and indefinite integrals from [**this**](https://www.khanacademy.org/math/calculus-home/integral-calculus/indefinite-integrals) page and some of those on [**this**](https://www.khanacademy.org/math/calculus-home/integral-calculus/fundamental-theorem-of-calculus-ic) page to understand the link between derivatives and integrals.
]


---
class:middle, center
# Index card:

## 4. How confident are you feeling about calculus intuition?
### 1 = no confidence, 5 = very confident

